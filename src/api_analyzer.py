#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
APIæ™ºèƒ½åˆ†ææ¨¡å— - ä¼˜åŒ–ç‰ˆ
Optimized API analysis module with batch processing
"""

import os
import json
import time
import requests
import pandas as pd
from pathlib import Path
import logging
from datetime import datetime
from typing import List, Dict, Any
import backoff

class APIAnalyzer:
    """APIæ™ºèƒ½åˆ†æå™¨ - æ‰¹é‡å¤„ç†ä¼˜åŒ–ç‰ˆ"""
    
    def __init__(self, config_dir='config', output_dir='output'):
        """åˆå§‹åŒ–APIåˆ†æå™¨"""
        self.config_dir = Path(config_dir)
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(exist_ok=True)
        
        # åŠ è½½é…ç½®
        self.config = self._load_config()
        self.request_times = []
        
        # é…ç½®æ—¥å¿—
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler('logs/api_analysis.log', encoding='utf-8'),
                logging.StreamHandler()
            ]
        )
        
        logging.info(f"APIåˆ†æå™¨åˆå§‹åŒ–å®Œæˆ - æ¨¡å‹: {self.config.get('model', 'N/A')}")
    
    def _load_config(self):
        """åŠ è½½APIé…ç½®"""
        config_file = self.config_dir / 'api_config.json'
        
        # é»˜è®¤é…ç½®
        default_config = {
            "api_key": os.getenv('SILICONFLOW_API_KEY', ''),
            "base_url": "https://api.siliconflow.cn/v1",
            "model": "deepseek-ai/DeepSeek-V3",
            "max_requests_per_minute": 50,
            "batch_size": 20,
            "timeout": 60
        }
        
        if config_file.exists():
            try:
                with open(config_file, 'r', encoding='utf-8') as f:
                    user_config = json.load(f)
                default_config.update(user_config)
            except Exception as e:
                logging.warning(f"é…ç½®æ–‡ä»¶åŠ è½½å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤é…ç½®: {e}")
        
        if not default_config['api_key']:
            logging.warning("æœªè®¾ç½®APIå¯†é’¥ï¼Œè¯·è®¾ç½®ç¯å¢ƒå˜é‡SILICONFLOW_API_KEY")
        
        return default_config
    
    def _rate_limit(self):
        """è¯·æ±‚é™æµ"""
        current_time = time.time()
        
        # æ¸…ç†è¶…è¿‡1åˆ†é’Ÿçš„è¯·æ±‚è®°å½•
        self.request_times = [t for t in self.request_times if current_time - t < 60]
        
        # æ£€æŸ¥æ˜¯å¦è¶…è¿‡é™åˆ¶
        max_requests = self.config.get('max_requests_per_minute', 20)
        if len(self.request_times) >= max_requests:
            sleep_time = 60 - (current_time - self.request_times[0])
            if sleep_time > 0:
                logging.info(f"è¾¾åˆ°è¯·æ±‚é™åˆ¶ï¼Œç­‰å¾… {sleep_time:.1f} ç§’")
                time.sleep(sleep_time)
        
        self.request_times.append(current_time)
    
    @backoff.on_exception(backoff.expo, requests.exceptions.RequestException, max_tries=3)
    def _make_api_request(self, business_texts: List[str], company_names: List[str]) -> Dict[str, Any]:
        """å‘é€æ‰¹é‡APIè¯·æ±‚"""
        self._rate_limit()
        
        # æ„å»ºæ‰¹é‡åˆ†ææç¤º
        batch_prompt = self._build_batch_prompt(business_texts, company_names)
        
        headers = {
            "Authorization": f"Bearer {self.config['api_key']}",
            "Content-Type": "application/json"
        }
        
        data = {
            "model": self.config['model'],
            "messages": [
                {
                    "role": "system",
                    "content": "ä½ æ˜¯ä¸“ä¸šçš„å•†ä¸šåˆ†æå¸ˆï¼Œæ“…é•¿åˆ†æå…¬å¸ä¸»è¥ä¸šåŠ¡å¹¶åˆ¤æ–­æ˜¯å¦ä¸æ—…æ¸¸è¡Œä¸šç›¸å…³ã€‚è¯·å¯¹æ¯å®¶å…¬å¸è¿›è¡Œåˆ†æï¼Œç»™å‡ºæ˜¯å¦å±äºæ—…æ¸¸ç›¸å…³è¡Œä¸šçš„åˆ¤æ–­å’Œè¯„åˆ†ã€‚"
                },
                {
                    "role": "user",
                    "content": batch_prompt
                }
            ],
            "temperature": 0.1,
            "max_tokens": 2000
        }
        
        try:
            response = requests.post(
                f"{self.config['base_url']}/chat/completions",
                headers=headers,
                json=data,
                timeout=self.config['timeout']
            )
            response.raise_for_status()
            return response.json()
        except Exception as e:
            logging.error(f"APIè¯·æ±‚å¤±è´¥: {e}")
            raise
    
    def _build_batch_prompt(self, business_texts: List[str], company_names: List[str]) -> str:
        """æ„å»ºæ‰¹é‡åˆ†ææç¤º"""
        prompt = "è¯·åˆ†æä»¥ä¸‹å…¬å¸çš„ä¸»è¥ä¸šåŠ¡ï¼Œåˆ¤æ–­æ˜¯å¦ä¸æ—…æ¸¸è¡Œä¸šç›¸å…³ï¼š\n\n"
        
        for i, (name, business) in enumerate(zip(company_names, business_texts), 1):
            prompt += f"{i}. å…¬å¸ï¼š{name}\n"
            prompt += f"   ä¸šåŠ¡ï¼š{business}\n\n"
        
        prompt += """
è¯·å¯¹æ¯å®¶å…¬å¸æŒ‰ä»¥ä¸‹æ ¼å¼å›ç­”ï¼š
å…¬å¸X: æ˜¯å¦æ—…æ¸¸ç›¸å…³(æ˜¯/å¦) | è¯„åˆ†(0-100) | åˆ†ç±» | ç†ç”±

è¦æ±‚ï¼š
1. è¯„åˆ†æ ‡å‡†ï¼š0-30åˆ†ä¸ºæ— å…³ï¼Œ31-60åˆ†ä¸ºé—´æ¥ç›¸å…³ï¼Œ61-100åˆ†ä¸ºç›´æ¥ç›¸å…³
2. åˆ†ç±»åŒ…æ‹¬ï¼šé…’åº—ä½å®¿ã€é¤é¥®æœåŠ¡ã€æ—…æ¸¸æœåŠ¡ã€äº¤é€šè¿è¾“ã€å¨±ä¹ä¼‘é—²ã€é›¶å”®å•†ä¸šã€ä¼šå±•æœåŠ¡ç­‰
3. ç†ç”±ç®€æ´æ˜äº†ï¼Œä¸è¶…è¿‡30å­—
"""
        return prompt
    
    def _parse_batch_response(self, response_content: str, company_names: List[str]) -> List[Dict[str, Any]]:
        """è§£ææ‰¹é‡å“åº”ç»“æœ"""
        results = []
        lines = response_content.split('\n')
        
        for i, company_name in enumerate(company_names):
            result = {
                'company_name': company_name,
                'is_tourism_related': False,
                'tourism_score': 0,
                'tourism_category': '',
                'analysis_reason': '',
                'api_response': response_content
            }
            
            # æŸ¥æ‰¾å¯¹åº”å…¬å¸çš„åˆ†æç»“æœ
            company_pattern = f"å…¬å¸{i+1}"
            for line in lines:
                if company_pattern in line and '|' in line:
                    try:
                        parts = line.split('|')
                        if len(parts) >= 4:
                            # è§£ææ˜¯å¦æ—…æ¸¸ç›¸å…³
                            tourism_part = parts[0].strip()
                            result['is_tourism_related'] = 'æ˜¯' in tourism_part
                            
                            # è§£æè¯„åˆ†
                            score_part = parts[1].strip()
                            import re
                            score_match = re.search(r'\d+', score_part)
                            if score_match:
                                result['tourism_score'] = int(score_match.group())
                            
                            # è§£æåˆ†ç±»å’Œç†ç”±
                            result['tourism_category'] = parts[2].strip()
                            result['analysis_reason'] = parts[3].strip()
                    except Exception as e:
                        logging.warning(f"è§£æå…¬å¸ {company_name} ç»“æœå¤±è´¥: {e}")
                    break
            
            results.append(result)
        
        return results
    
    def _deduplicate_business_texts(self, business_data: List[Dict[str, str]]) -> tuple:
        """å¯¹ä¸šåŠ¡æ–‡æœ¬å»é‡ï¼Œè¿”å›å»é‡åçš„æ•°æ®å’Œæ˜ å°„å…³ç³»

        Returns:
            tuple: (unique_business_data, text_to_records_map, dedup_stats)
        """
        text_to_records = {}
        unique_texts = {}

        # æŒ‰ä¸šåŠ¡æ–‡æœ¬åˆ†ç»„
        for i, record in enumerate(business_data):
            business_text = record.get('business_text', '').strip()

            if business_text not in text_to_records:
                text_to_records[business_text] = []
                unique_texts[business_text] = record  # ä¿å­˜ç¬¬ä¸€ä¸ªå‡ºç°çš„è®°å½•ä½œä¸ºä»£è¡¨

            text_to_records[business_text].append((i, record))

        # åˆ›å»ºå»é‡åçš„æ•°æ®åˆ—è¡¨
        unique_business_data = []
        text_to_index_map = {}

        for text, representative_record in unique_texts.items():
            text_to_index_map[text] = len(unique_business_data)
            unique_business_data.append(representative_record)

        # ç»Ÿè®¡ä¿¡æ¯
        original_count = len(business_data)
        unique_count = len(unique_business_data)
        duplicate_count = original_count - unique_count

        dedup_stats = {
            'original_count': original_count,
            'unique_count': unique_count,
            'duplicate_count': duplicate_count,
            'dedup_ratio': duplicate_count / original_count if original_count > 0 else 0
        }

        logging.info(f"ä¸šåŠ¡æ–‡æœ¬å»é‡å®Œæˆ: {original_count} -> {unique_count} "
                    f"(å»é‡ {duplicate_count} æ¡, æ¯”ä¾‹ {dedup_stats['dedup_ratio']:.1%})")

        return unique_business_data, text_to_records, dedup_stats

    def _apply_results_to_duplicates(self, unique_results: List[Dict[str, Any]],
                                   text_to_records: Dict[str, List],
                                   original_business_data: List[Dict[str, str]]) -> List[Dict[str, Any]]:
        """å°†å»é‡åˆ†æçš„ç»“æœåº”ç”¨åˆ°æ‰€æœ‰é‡å¤è®°å½•ä¸Š"""
        all_results = [None] * len(original_business_data)

        for result in unique_results:
            business_text = result.get('business_text', '').strip()

            if business_text in text_to_records:
                # å°†ç»“æœåº”ç”¨åˆ°æ‰€æœ‰å…·æœ‰ç›¸åŒä¸šåŠ¡æ–‡æœ¬çš„è®°å½•
                for original_index, original_record in text_to_records[business_text]:
                    # åˆ›å»ºæ–°çš„ç»“æœè®°å½•ï¼Œä¿ç•™åŸå§‹è®°å½•çš„æ‰€æœ‰å­—æ®µ
                    combined_result = {**original_record}

                    # æ·»åŠ åˆ†æç»“æœå­—æ®µ
                    analysis_fields = [
                        'is_tourism_related', 'tourism_score', 'tourism_category',
                        'analysis_reason', 'api_response', 'timestamp', 'batch_number'
                    ]

                    for field in analysis_fields:
                        if field in result:
                            combined_result[field] = result[field]

                    # æ ‡è®°è¿™æ˜¯ä»å»é‡ç»“æœå¤åˆ¶çš„
                    combined_result['is_deduplicated'] = True

                    all_results[original_index] = combined_result

        # è¿‡æ»¤æ‰Noneå€¼ï¼ˆç†è®ºä¸Šä¸åº”è¯¥æœ‰ï¼‰
        final_results = [r for r in all_results if r is not None]

        logging.info(f"å»é‡ç»“æœåº”ç”¨å®Œæˆ: {len(unique_results)} ä¸ªå”¯ä¸€ç»“æœ -> {len(final_results)} æ¡æœ€ç»ˆè®°å½•")

        return final_results

    def analyze_batch_with_incremental_save(self, business_data: List[Dict[str, str]],
                                           progress_file: str = None,
                                           enable_deduplication: bool = True) -> List[Dict[str, Any]]:
        """æ‰¹é‡åˆ†æä¸šåŠ¡æ–‡æœ¬ï¼Œæ¯æ‰¹æ¬¡åç«‹å³ä¿å­˜ç»“æœï¼Œæ”¯æŒå»é‡ä¼˜åŒ–"""
        if not self.config['api_key']:
            logging.error("APIå¯†é’¥æœªè®¾ç½®ï¼Œæ— æ³•è¿›è¡Œåˆ†æ")
            return []

        # ä¿å­˜åŸå§‹æ•°æ®ç”¨äºæœ€ç»ˆç»“æœæ˜ å°„
        original_business_data = business_data.copy()

        # å»é‡å¤„ç†
        if enable_deduplication and len(business_data) > 1:
            unique_business_data, text_to_records, dedup_stats = self._deduplicate_business_texts(business_data)
            business_data_to_process = unique_business_data

            logging.info(f"å¯ç”¨å»é‡ä¼˜åŒ–: åŸå§‹ {dedup_stats['original_count']} æ¡ -> "
                        f"éœ€å¤„ç† {dedup_stats['unique_count']} æ¡ "
                        f"(èŠ‚çœ {dedup_stats['dedup_ratio']:.1%} APIè°ƒç”¨)")
        else:
            business_data_to_process = business_data
            text_to_records = None
            dedup_stats = None
            logging.info("æœªå¯ç”¨å»é‡ä¼˜åŒ–ï¼Œå°†å¤„ç†å…¨éƒ¨æ•°æ®")

        batch_size = self.config.get('batch_size', 8)
        all_results = []

        # è®¾ç½®è¿›åº¦æ–‡ä»¶è·¯å¾„
        if not progress_file:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            progress_file = self.output_dir / f"api_analysis_progress_{timestamp}.csv"
        else:
            progress_file = Path(progress_file)

        # åŠ è½½å·²æœ‰çš„è¿›åº¦æ–‡ä»¶ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        if progress_file.exists():
            try:
                existing_df = pd.read_csv(progress_file, encoding='utf-8')
                all_results = existing_df.to_dict('records')
                logging.info(f"ä»è¿›åº¦æ–‡ä»¶åŠ è½½äº† {len(all_results)} æ¡å·²å¤„ç†è®°å½•")
            except Exception as e:
                logging.warning(f"åŠ è½½è¿›åº¦æ–‡ä»¶å¤±è´¥: {e}")

        logging.info(f"å¼€å§‹æ‰¹é‡åˆ†æ {len(business_data_to_process)} æ¡å»é‡åè®°å½•ï¼Œæ‰¹æ¬¡å¤§å°: {batch_size}")
        logging.info(f"è¿›åº¦æ–‡ä»¶: {progress_file}")

        # å¤„ç†å»é‡åçš„æ•°æ®
        unique_results = []

        for i in range(0, len(business_data_to_process), batch_size):
            batch = business_data_to_process[i:i+batch_size]
            batch_num = i//batch_size + 1
            total_batches = (len(business_data_to_process)-1)//batch_size + 1

            try:
                # æå–æ‰¹æ¬¡æ•°æ®
                business_texts = [item.get('business_text', '') for item in batch]
                company_names = [item.get('company_name', '') for item in batch]

                logging.info(f"å¤„ç†å»é‡æ‰¹æ¬¡ {batch_num}/{total_batches} (è®°å½• {i+1}-{min(i+batch_size, len(business_data_to_process))})")

                # å‘é€APIè¯·æ±‚
                response = self._make_api_request(business_texts, company_names)
                content = response.get('choices', [{}])[0].get('message', {}).get('content', '')

                # è§£æç»“æœ
                batch_results = self._parse_batch_response(content, company_names)

                # åˆå¹¶åŸå§‹æ•°æ®
                batch_combined_results = []
                for j, result in enumerate(batch_results):
                    original_data = batch[j]
                    combined_result = {**original_data, **result}
                    combined_result['timestamp'] = datetime.now().isoformat()
                    combined_result['batch_number'] = batch_num
                    combined_result['is_deduplicated'] = False  # æ ‡è®°ä¸ºåŸå§‹åˆ†æç»“æœ
                    batch_combined_results.append(combined_result)

                # æ·»åŠ åˆ°å»é‡ç»“æœ
                unique_results.extend(batch_combined_results)

                logging.info(f"å»é‡æ‰¹æ¬¡ {batch_num} å®Œæˆï¼Œåˆ†æäº† {len(batch_combined_results)} æ¡å”¯ä¸€è®°å½•")

                # çŸ­æš‚å»¶è¿Ÿ
                time.sleep(1)

            except Exception as e:
                logging.error(f"å»é‡æ‰¹æ¬¡ {batch_num} å¤„ç†å¤±è´¥: {e}")
                # æ·»åŠ é”™è¯¯è®°å½•
                error_results = []
                for item in batch:
                    error_result = {
                        **item,
                        'error': str(e),
                        'timestamp': datetime.now().isoformat(),
                        'batch_number': batch_num,
                        'is_deduplicated': False
                    }
                    error_results.append(error_result)

                unique_results.extend(error_results)
                logging.info(f"å»é‡æ‰¹æ¬¡ {batch_num} é”™è¯¯å·²è®°å½•")

        # å¦‚æœå¯ç”¨äº†å»é‡ï¼Œå°†ç»“æœåº”ç”¨åˆ°æ‰€æœ‰é‡å¤è®°å½•
        if enable_deduplication and text_to_records:
            final_results = self._apply_results_to_duplicates(unique_results, text_to_records, original_business_data)
            logging.info(f"å»é‡ç»“æœå·²åº”ç”¨åˆ°æ‰€æœ‰ {len(final_results)} æ¡åŸå§‹è®°å½•")
        else:
            final_results = unique_results

        # åˆå¹¶å·²æœ‰ç»“æœï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
        if all_results:
            # ç§»é™¤é‡å¤çš„å·²æœ‰ç»“æœ
            existing_ids = set()
            for result in all_results:
                record_id = self._create_record_id(
                    result.get('company_name', ''),
                    result.get('business_text', ''),
                    result.get('symbol', '')
                )
                existing_ids.add(record_id)

            # åªæ·»åŠ æ–°çš„ç»“æœ
            new_results = []
            for result in final_results:
                record_id = self._create_record_id(
                    result.get('company_name', ''),
                    result.get('business_text', ''),
                    result.get('symbol', '')
                )
                if record_id not in existing_ids:
                    new_results.append(result)

            final_results = all_results + new_results
            logging.info(f"åˆå¹¶ç»“æœ: å·²æœ‰ {len(all_results)} + æ–°å¢ {len(new_results)} = æ€»è®¡ {len(final_results)}")

        # ä¿å­˜æœ€ç»ˆè¿›åº¦
        self._save_progress(final_results, progress_file)

        logging.info(f"æ‰¹é‡åˆ†æå®Œæˆï¼Œå…±å¤„ç† {len(final_results)} æ¡è®°å½•")
        if dedup_stats:
            logging.info(f"å»é‡ä¼˜åŒ–æ•ˆæœ: èŠ‚çœäº† {dedup_stats['duplicate_count']} æ¬¡APIè°ƒç”¨ "
                        f"({dedup_stats['dedup_ratio']:.1%})")
        logging.info(f"æœ€ç»ˆè¿›åº¦æ–‡ä»¶: {progress_file}")

        return final_results, str(progress_file)

    def _save_progress(self, results: List[Dict[str, Any]], progress_file: Path):
        """ä¿å­˜è¿›åº¦åˆ°æ–‡ä»¶"""
        try:
            results_df = pd.DataFrame(results)
            results_df.to_csv(progress_file, index=False, encoding='utf-8-sig')
            logging.debug(f"è¿›åº¦å·²ä¿å­˜: {len(results)} æ¡è®°å½• -> {progress_file}")
        except Exception as e:
            logging.error(f"ä¿å­˜è¿›åº¦æ–‡ä»¶å¤±è´¥: {e}")

    def analyze_batch(self, business_data: List[Dict[str, str]]) -> List[Dict[str, Any]]:
        """æ‰¹é‡åˆ†æä¸šåŠ¡æ–‡æœ¬ï¼ˆå…¼å®¹æ€§æ–¹æ³•ï¼‰"""
        results, _ = self.analyze_batch_with_incremental_save(business_data)
        return results
    
    def _load_processed_records(self, csv_file: str = None) -> set:
        """åŠ è½½å·²å¤„ç†çš„è®°å½•ï¼Œè¿”å›å·²å¤„ç†çš„å…¬å¸æ ‡è¯†ç¬¦é›†åˆ

        Args:
            csv_file: åŸå§‹CSVæ–‡ä»¶è·¯å¾„ï¼ˆç”¨äºæ—¥å¿—è®°å½•ï¼‰
        """
        processed_ids = set()

        # æŸ¥æ‰¾æœ€æ–°çš„ç»“æœæ–‡ä»¶
        pattern = f"api_analysis_results_*.csv"
        result_files = list(self.output_dir.glob(pattern))

        if not result_files:
            logging.info("æœªæ‰¾åˆ°å·²å¤„ç†çš„ç»“æœæ–‡ä»¶ï¼Œå°†ä»å¤´å¼€å§‹å¤„ç†")
            return processed_ids

        # æŒ‰æ—¶é—´æˆ³æ’åºï¼Œè·å–æœ€æ–°çš„æ–‡ä»¶
        latest_file = max(result_files, key=lambda x: x.stat().st_mtime)

        try:
            processed_df = pd.read_csv(latest_file, encoding='utf-8')

            # ä½¿ç”¨å…¬å¸åç§°+ä¸šåŠ¡æè¿°çš„ç»„åˆä½œä¸ºå”¯ä¸€æ ‡è¯†ç¬¦
            for _, row in processed_df.iterrows():
                company_name = str(row.get('company_name', ''))
                business_text = str(row.get('business_text', ''))
                symbol = str(row.get('symbol', ''))

                # åˆ›å»ºå”¯ä¸€æ ‡è¯†ç¬¦
                record_id = f"{company_name}|{business_text}|{symbol}"
                processed_ids.add(record_id)

            source_info = f"æ¥æºæ–‡ä»¶: {csv_file}" if csv_file else ""
            logging.info(f"ä» {latest_file.name} åŠ è½½äº† {len(processed_ids)} æ¡å·²å¤„ç†è®°å½• {source_info}")

        except Exception as e:
            logging.warning(f"åŠ è½½å·²å¤„ç†è®°å½•å¤±è´¥: {e}")

        return processed_ids

    def _create_record_id(self, company_name: str, business_text: str, symbol: str) -> str:
        """åˆ›å»ºè®°å½•çš„å”¯ä¸€æ ‡è¯†ç¬¦"""
        return f"{company_name}|{business_text}|{symbol}"

    def _filter_unprocessed_data(self, business_data: List[Dict[str, str]], processed_ids: set) -> List[Dict[str, str]]:
        """è¿‡æ»¤å‡ºæœªå¤„ç†çš„æ•°æ®"""
        unprocessed_data = []

        for item in business_data:
            record_id = self._create_record_id(
                item.get('company_name', ''),
                item.get('business_text', ''),
                item.get('symbol', '')
            )

            if record_id not in processed_ids:
                unprocessed_data.append(item)

        return unprocessed_data

    def _find_latest_progress_file(self) -> Path:
        """æŸ¥æ‰¾æœ€æ–°çš„è¿›åº¦æ–‡ä»¶"""
        pattern = f"api_analysis_progress_*.csv"
        progress_files = list(self.output_dir.glob(pattern))

        if progress_files:
            latest_file = max(progress_files, key=lambda x: x.stat().st_mtime)
            return latest_file
        return None

    def _load_processed_records_from_progress(self) -> tuple:
        """ä»è¿›åº¦æ–‡ä»¶åŠ è½½å·²å¤„ç†çš„è®°å½•

        Returns:
            tuple: (processed_ids_set, progress_file_path, existing_results_list)
        """
        processed_ids = set()
        existing_results = []
        progress_file = self._find_latest_progress_file()

        if not progress_file or not progress_file.exists():
            logging.info("æœªæ‰¾åˆ°è¿›åº¦æ–‡ä»¶ï¼Œå°†ä»å¤´å¼€å§‹å¤„ç†")
            return processed_ids, None, existing_results

        try:
            progress_df = pd.read_csv(progress_file, encoding='utf-8')
            existing_results = progress_df.to_dict('records')

            # åˆ›å»ºå·²å¤„ç†è®°å½•çš„IDé›†åˆ
            for result in existing_results:
                company_name = str(result.get('company_name', ''))
                business_text = str(result.get('business_text', ''))
                symbol = str(result.get('symbol', ''))
                record_id = self._create_record_id(company_name, business_text, symbol)
                processed_ids.add(record_id)

            logging.info(f"ä»è¿›åº¦æ–‡ä»¶ {progress_file.name} åŠ è½½äº† {len(processed_ids)} æ¡å·²å¤„ç†è®°å½•")

        except Exception as e:
            logging.warning(f"åŠ è½½è¿›åº¦æ–‡ä»¶å¤±è´¥: {e}")
            return set(), None, []

        return processed_ids, str(progress_file), existing_results

    def analyze_csv_file_incremental(self, csv_file: str, sample_size: int = None,
                                    force_restart: bool = False, enable_deduplication: bool = True) -> str:
        """å¢é‡åˆ†æCSVæ–‡ä»¶ï¼Œæ”¯æŒæ–­ç‚¹ç»­ä¼ """
        logging.info(f"å¼€å§‹å¢é‡åˆ†æCSVæ–‡ä»¶: {csv_file}")

        # è¯»å–æ•°æ®
        df = pd.read_csv(csv_file, encoding='utf-8')
        logging.info(f"è¯»å–åˆ° {len(df)} æ¡è®°å½•")

        # é‡‡æ ·ï¼ˆå¦‚æœæŒ‡å®šäº†sample_sizeä¸”å°äºæ€»æ•°æ®é‡ï¼‰
        if sample_size and sample_size < len(df):
            df = df.sample(n=sample_size, random_state=42)
            logging.info(f"éšæœºé‡‡æ · {sample_size} æ¡è®°å½•è¿›è¡Œåˆ†æ")

        # å‡†å¤‡åˆ†ææ•°æ®
        business_data = []
        for _, row in df.iterrows():
            business_data.append({
                'company_name': str(row.get('ShortName', '')),
                'business_text': str(row.get('MAINBUSSINESS', '')),
                'symbol': str(row.get('Symbol', '')),
                'end_date': str(row.get('EndDate', ''))
            })

        # å¤„ç†æ–­ç‚¹ç»­ä¼ é€»è¾‘
        progress_file = None
        existing_results = []

        if not force_restart:
            # ä»è¿›åº¦æ–‡ä»¶åŠ è½½å·²å¤„ç†çš„è®°å½•
            processed_ids, progress_file, existing_results = self._load_processed_records_from_progress()
            unprocessed_data = self._filter_unprocessed_data(business_data, processed_ids)

            if not unprocessed_data:
                logging.info("æ‰€æœ‰è®°å½•éƒ½å·²å¤„ç†å®Œæˆï¼Œæ— éœ€é‡å¤åˆ†æ")
                if progress_file:
                    # å°†è¿›åº¦æ–‡ä»¶è½¬æ¢ä¸ºæœ€ç»ˆç»“æœæ–‡ä»¶
                    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                    final_file = self.output_dir / f"api_analysis_results_{timestamp}.csv"

                    results_df = pd.DataFrame(existing_results)
                    results_df.to_csv(final_file, index=False, encoding='utf-8-sig')

                    # ç”ŸæˆæŠ¥å‘Š
                    self._generate_api_report(existing_results, timestamp)

                    logging.info(f"æ‰€æœ‰æ•°æ®å·²å¤„ç†å®Œæˆï¼Œæœ€ç»ˆç»“æœä¿å­˜ä¸º: {final_file}")
                    return str(final_file)
                else:
                    logging.warning("æœªæ‰¾åˆ°ç»“æœæ–‡ä»¶")
                    return ""

            logging.info(f"å‘ç° {len(unprocessed_data)} æ¡æœªå¤„ç†è®°å½•ï¼Œå°†ç»§ç»­å¤„ç†")
            business_data = unprocessed_data
        else:
            logging.info("å¼ºåˆ¶é‡æ–°å¼€å§‹ï¼Œå°†å¤„ç†å…¨éƒ¨æ•°æ®")
            # åˆ é™¤æ—§çš„è¿›åº¦æ–‡ä»¶
            old_progress_files = list(self.output_dir.glob("api_analysis_progress_*.csv"))
            for old_file in old_progress_files:
                try:
                    old_file.unlink()
                    logging.info(f"åˆ é™¤æ—§è¿›åº¦æ–‡ä»¶: {old_file}")
                except Exception as e:
                    logging.warning(f"åˆ é™¤æ—§è¿›åº¦æ–‡ä»¶å¤±è´¥: {e}")

        # æ‰§è¡Œæ‰¹é‡åˆ†æï¼ˆä½¿ç”¨å¢é‡ä¿å­˜å’Œå»é‡ä¼˜åŒ–ï¼‰
        new_results, final_progress_file = self.analyze_batch_with_incremental_save(
            business_data, progress_file, enable_deduplication
        )

        # ç”Ÿæˆæœ€ç»ˆç»“æœæ–‡ä»¶
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        final_result_file = self.output_dir / f"api_analysis_results_{timestamp}.csv"

        # åˆå¹¶æ‰€æœ‰ç»“æœï¼ˆå·²æœ‰çš„ + æ–°çš„ï¼‰
        if not force_restart and existing_results:
            # ç§»é™¤é‡å¤çš„å·²æœ‰ç»“æœï¼ˆå› ä¸ºnew_resultså¯èƒ½åŒ…å«äº†æ‰€æœ‰ç»“æœï¼‰
            all_results = existing_results + [r for r in new_results if r not in existing_results]
        else:
            all_results = new_results

        # ä¿å­˜æœ€ç»ˆç»“æœ
        results_df = pd.DataFrame(all_results)
        results_df.to_csv(final_result_file, index=False, encoding='utf-8-sig')

        # ç”Ÿæˆç»Ÿè®¡æŠ¥å‘Š
        self._generate_api_report(all_results, timestamp)

        # è‡ªåŠ¨æ£€æµ‹å¹¶åˆå¹¶åŸºç¡€åˆ†æç»“æœ
        self._auto_merge_with_basic_analysis(str(final_result_file))

        logging.info(f"å¢é‡åˆ†æå®Œæˆï¼")
        logging.info(f"è¿›åº¦æ–‡ä»¶: {final_progress_file}")
        logging.info(f"æœ€ç»ˆç»“æœ: {final_result_file}")
        logging.info(f"æœ¬æ¬¡æ–°å¢åˆ†æ: {len(business_data)} æ¡ï¼Œæ€»è®¡: {len(all_results)} æ¡")

        return str(final_result_file)

    def get_processing_status(self, csv_file: str, sample_size: int = None) -> Dict[str, Any]:
        """è·å–å¤„ç†çŠ¶æ€ä¿¡æ¯"""
        # è¯»å–åŸå§‹æ•°æ®
        df = pd.read_csv(csv_file, encoding='utf-8')
        total_records = len(df)

        # å¦‚æœæŒ‡å®šäº†é‡‡æ ·å¤§å°
        if sample_size and sample_size < total_records:
            target_records = sample_size
        else:
            target_records = total_records

        # ä¼˜å…ˆä»è¿›åº¦æ–‡ä»¶åŠ è½½å·²å¤„ç†çš„è®°å½•
        processed_ids, progress_file, existing_results = self._load_processed_records_from_progress()

        # å¦‚æœæ²¡æœ‰è¿›åº¦æ–‡ä»¶ï¼Œå°è¯•ä»ç»“æœæ–‡ä»¶åŠ è½½
        if not processed_ids:
            processed_ids = self._load_processed_records(csv_file)

        processed_count = len(processed_ids)

        # è®¡ç®—è¿›åº¦
        progress_percentage = (processed_count / target_records * 100) if target_records > 0 else 0
        remaining_count = max(0, target_records - processed_count)

        status = {
            'total_records': total_records,
            'target_records': target_records,
            'processed_count': processed_count,
            'remaining_count': remaining_count,
            'progress_percentage': round(progress_percentage, 2),
            'is_complete': remaining_count == 0,
            'progress_file': progress_file,
            'latest_result_file': None,
            'last_batch_number': None
        }

        # è·å–æœ€åå¤„ç†çš„æ‰¹æ¬¡å·
        if existing_results:
            batch_numbers = [r.get('batch_number', 0) for r in existing_results if 'batch_number' in r]
            if batch_numbers:
                status['last_batch_number'] = max(batch_numbers)

        # æŸ¥æ‰¾æœ€æ–°ç»“æœæ–‡ä»¶
        pattern = f"api_analysis_results_*.csv"
        result_files = list(self.output_dir.glob(pattern))
        if result_files:
            latest_file = max(result_files, key=lambda x: x.stat().st_mtime)
            status['latest_result_file'] = str(latest_file)

        return status

    def analyze_csv_file(self, csv_file: str, sample_size: int = None,
                        incremental: bool = True, enable_deduplication: bool = True) -> str:
        """åˆ†æCSVæ–‡ä»¶

        Args:
            csv_file: CSVæ–‡ä»¶è·¯å¾„
            sample_size: é‡‡æ ·å¤§å°ï¼ŒNoneè¡¨ç¤ºå…¨éƒ¨æ•°æ®
            incremental: æ˜¯å¦å¯ç”¨å¢é‡å¤„ç†ï¼ˆé»˜è®¤Trueï¼‰
            enable_deduplication: æ˜¯å¦å¯ç”¨å»é‡ä¼˜åŒ–ï¼ˆé»˜è®¤Trueï¼‰
        """
        if incremental:
            return self.analyze_csv_file_incremental(csv_file, sample_size, force_restart=False,
                                                   enable_deduplication=enable_deduplication)
        else:
            # åŸæœ‰çš„å®Œæ•´å¤„ç†é€»è¾‘
            logging.info(f"å¼€å§‹å®Œæ•´åˆ†æCSVæ–‡ä»¶: {csv_file}")

            # è¯»å–æ•°æ®
            df = pd.read_csv(csv_file, encoding='utf-8')
            logging.info(f"è¯»å–åˆ° {len(df)} æ¡è®°å½•")

            # é‡‡æ ·ï¼ˆå¦‚æœæŒ‡å®šäº†sample_sizeä¸”å°äºæ€»æ•°æ®é‡ï¼‰
            if sample_size and sample_size < len(df):
                df = df.sample(n=sample_size, random_state=42)
                logging.info(f"éšæœºé‡‡æ · {sample_size} æ¡è®°å½•è¿›è¡Œåˆ†æ")
            else:
                logging.info(f"å°†åˆ†æå…¨éƒ¨ {len(df)} æ¡è®°å½•")

            # å‡†å¤‡åˆ†ææ•°æ®
            business_data = []
            for _, row in df.iterrows():
                business_data.append({
                    'company_name': str(row.get('ShortName', '')),
                    'business_text': str(row.get('MAINBUSSINESS', '')),
                    'symbol': str(row.get('Symbol', '')),
                    'end_date': str(row.get('EndDate', ''))
                })

            # æ‰§è¡Œæ‰¹é‡åˆ†æ
            results = self.analyze_batch(business_data)

            # ä¿å­˜ç»“æœ
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

            # ä¿å­˜ä¸ºCSV
            results_df = pd.DataFrame(results)
            csv_file_path = self.output_dir / f"api_analysis_results_{timestamp}.csv"
            results_df.to_csv(csv_file_path, index=False, encoding='utf-8-sig')

            # ç”Ÿæˆç»Ÿè®¡æŠ¥å‘Š
            self._generate_api_report(results, timestamp)

            # è‡ªåŠ¨æ£€æµ‹å¹¶åˆå¹¶åŸºç¡€åˆ†æç»“æœ
            self._auto_merge_with_basic_analysis(str(csv_file_path))

            logging.info(f"APIåˆ†æç»“æœå·²ä¿å­˜: {csv_file_path}")
            return str(csv_file_path)
    
    def _generate_api_report(self, results: List[Dict[str, Any]], timestamp: str):
        """ç”ŸæˆAPIåˆ†ææŠ¥å‘Š"""
        report_file = self.output_dir / f"api_analysis_report_{timestamp}.md"
        
        # ç»Ÿè®¡ä¿¡æ¯
        total_count = len(results)
        tourism_count = sum(1 for r in results if r.get('is_tourism_related', False))
        error_count = sum(1 for r in results if 'error' in r)
        
        # è¯„åˆ†åˆ†å¸ƒ
        scores = [r.get('tourism_score', 0) for r in results if 'tourism_score' in r]
        avg_score = sum(scores) / len(scores) if scores else 0
        
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write(f"# APIæ™ºèƒ½åˆ†ææŠ¥å‘Š\n\n")
            f.write(f"**åˆ†ææ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"**ä½¿ç”¨æ¨¡å‹**: {self.config['model']}\n\n")
            
            f.write(f"## ç»Ÿè®¡æ¦‚è§ˆ\n")
            f.write(f"- æ€»åˆ†ææ•°é‡: {total_count}\n")
            f.write(f"- æ—…æ¸¸ç›¸å…³ä¼ä¸š: {tourism_count} ({tourism_count/total_count*100:.1f}%)\n")
            f.write(f"- åˆ†æå¤±è´¥: {error_count}\n")
            f.write(f"- å¹³å‡æ—…æ¸¸ç›¸å…³åº¦: {avg_score:.1f}åˆ†\n\n")
            
            # é«˜åˆ†ä¼ä¸š
            high_score_companies = [
                r for r in results 
                if r.get('tourism_score', 0) >= 80 and r.get('is_tourism_related', False)
            ]
            
            if high_score_companies:
                f.write(f"## é«˜åˆ†æ—…æ¸¸ä¼ä¸š (â‰¥80åˆ†)\n")
                for company in sorted(high_score_companies, key=lambda x: x.get('tourism_score', 0), reverse=True):
                    f.write(f"- **{company.get('company_name', 'N/A')}** (è¯„åˆ†: {company.get('tourism_score', 0)})\n")
                    f.write(f"  - åˆ†ç±»: {company.get('tourism_category', 'N/A')}\n")
                    f.write(f"  - ç†ç”±: {company.get('analysis_reason', 'N/A')}\n\n")
        
        logging.info(f"APIåˆ†ææŠ¥å‘Šå·²ç”Ÿæˆ: {report_file}")

    def _auto_merge_with_basic_analysis(self, api_results_file: str):
        """è‡ªåŠ¨æ£€æµ‹å¹¶åˆå¹¶åŸºç¡€åˆ†æç»“æœ"""
        try:
            # æ£€æŸ¥æ˜¯å¦å­˜åœ¨åŸºç¡€åˆ†æç»“æœæ–‡ä»¶
            base_file = self.output_dir / 'data_with_tourism_flag.csv'

            if not base_file.exists():
                logging.info("æœªæ‰¾åˆ°åŸºç¡€åˆ†æç»“æœæ–‡ä»¶ï¼Œè·³è¿‡è‡ªåŠ¨åˆå¹¶")
                return

            logging.info("æ£€æµ‹åˆ°åŸºç¡€åˆ†æç»“æœï¼Œå¼€å§‹è‡ªåŠ¨åˆå¹¶...")

            # å¯¼å…¥BusinessAnalyzerç±»
            from .business_analyzer import BusinessAnalyzer

            # åˆ›å»ºBusinessAnalyzerå®ä¾‹å¹¶æ‰§è¡Œåˆå¹¶
            business_analyzer = BusinessAnalyzer(output_dir=str(self.output_dir))
            merged_file = business_analyzer.merge_api_results(
                base_file=str(base_file),
                api_results_file=api_results_file
            )

            logging.info(f"è‡ªåŠ¨åˆå¹¶å®Œæˆï¼Œåˆå¹¶æ–‡ä»¶: {merged_file}")
            print(f"ğŸ”— è‡ªåŠ¨åˆå¹¶å®Œæˆï¼åˆå¹¶æ–‡ä»¶: {merged_file}")

        except Exception as e:
            logging.warning(f"è‡ªåŠ¨åˆå¹¶å¤±è´¥: {e}")
            print(f"âš ï¸  è‡ªåŠ¨åˆå¹¶å¤±è´¥: {e}")
            print("ğŸ’¡ æ‚¨å¯ä»¥ç¨åæ‰‹åŠ¨è¿è¡Œåˆå¹¶åŠŸèƒ½")

    def print_processing_status(self, csv_file: str, sample_size: int = None):
        """æ‰“å°å¤„ç†çŠ¶æ€ä¿¡æ¯"""
        status = self.get_processing_status(csv_file, sample_size)

        print("=" * 70)
        print("ğŸ“Š APIåˆ†æå¤„ç†çŠ¶æ€")
        print("=" * 70)
        print(f"ğŸ“ æ•°æ®æ–‡ä»¶: {csv_file}")
        print(f"ğŸ“ˆ æ€»è®°å½•æ•°: {status['total_records']:,}")
        print(f"ğŸ¯ ç›®æ ‡å¤„ç†æ•°: {status['target_records']:,}")
        print(f"âœ… å·²å¤„ç†æ•°: {status['processed_count']:,}")
        print(f"â³ å‰©ä½™æ•°é‡: {status['remaining_count']:,}")
        print(f"ğŸ“Š å®Œæˆè¿›åº¦: {status['progress_percentage']:.1f}%")
        print(f"ğŸ æ˜¯å¦å®Œæˆ: {'æ˜¯' if status['is_complete'] else 'å¦'}")

        # æ˜¾ç¤ºè¿›åº¦æ–‡ä»¶ä¿¡æ¯
        if status['progress_file']:
            print(f"ğŸ’¾ è¿›åº¦æ–‡ä»¶: {status['progress_file']}")
            if status['last_batch_number']:
                print(f"ğŸ”¢ æœ€åæ‰¹æ¬¡: ç¬¬ {status['last_batch_number']} æ‰¹")
        else:
            print("ğŸ’¾ è¿›åº¦æ–‡ä»¶: æš‚æ— ")

        # æ˜¾ç¤ºæœ€ç»ˆç»“æœæ–‡ä»¶
        if status['latest_result_file']:
            print(f"ğŸ“„ æœ€æ–°ç»“æœ: {status['latest_result_file']}")
        else:
            print("ğŸ“„ æœ€æ–°ç»“æœ: æš‚æ— ")

        # æ˜¾ç¤ºè¿›åº¦æ¡
        if status['target_records'] > 0:
            progress_bar_length = 40
            filled_length = int(progress_bar_length * status['processed_count'] / status['target_records'])
            bar = 'â–ˆ' * filled_length + 'â–‘' * (progress_bar_length - filled_length)
            print(f"ğŸ“ˆ è¿›åº¦æ¡: |{bar}| {status['progress_percentage']:.1f}%")

        print("=" * 70)

        return status

def main():
    """ä¸»å‡½æ•° - æ”¯æŒå¢é‡å¤„ç†å’ŒçŠ¶æ€æŸ¥çœ‹"""
    import argparse

    parser = argparse.ArgumentParser(description='APIæ™ºèƒ½åˆ†æå·¥å…· - æ”¯æŒå¢é‡å¤„ç†')
    parser.add_argument('csv_file', nargs='?', default='data/STK_LISTEDCOINFOANL.csv',
                       help='CSVæ•°æ®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--action', choices=['analyze', 'status', 'continue'], default='analyze',
                       help='æ“ä½œç±»å‹: analyze(åˆ†æ), status(æŸ¥çœ‹çŠ¶æ€), continue(ç»§ç»­å¤„ç†)')
    parser.add_argument('--sample-size', type=int, default=None,
                       help='é‡‡æ ·å¤§å°ï¼Œé»˜è®¤å¤„ç†å…¨éƒ¨æ•°æ®')
    parser.add_argument('--force-restart', action='store_true',
                       help='å¼ºåˆ¶é‡æ–°å¼€å§‹ï¼Œå¿½ç•¥å·²å¤„ç†çš„æ•°æ®')
    parser.add_argument('--no-incremental', action='store_true',
                       help='ç¦ç”¨å¢é‡å¤„ç†ï¼Œæ¯æ¬¡éƒ½å®Œæ•´å¤„ç†')
    parser.add_argument('--no-deduplication', action='store_true',
                       help='ç¦ç”¨å»é‡ä¼˜åŒ–ï¼Œå¤„ç†æ‰€æœ‰é‡å¤æ–‡æœ¬')

    args = parser.parse_args()

    analyzer = APIAnalyzer()

    if args.action == 'status':
        # æŸ¥çœ‹å¤„ç†çŠ¶æ€
        analyzer.print_processing_status(args.csv_file, args.sample_size)

    elif args.action == 'continue':
        # ç»§ç»­å¤„ç†æœªå®Œæˆçš„æ•°æ®
        print("ğŸ”„ ç»§ç»­å¤„ç†æœªå®Œæˆçš„æ•°æ®...")
        enable_dedup = not args.no_deduplication
        if enable_dedup:
            print("ğŸ“Š å¯ç”¨å»é‡ä¼˜åŒ–ï¼Œç›¸åŒä¸šåŠ¡æ–‡æœ¬åªåˆ†æä¸€æ¬¡")
        else:
            print("âš ï¸  å·²ç¦ç”¨å»é‡ä¼˜åŒ–ï¼Œå°†åˆ†ææ‰€æœ‰é‡å¤æ–‡æœ¬")

        result_file = analyzer.analyze_csv_file_incremental(
            args.csv_file,
            args.sample_size,
            force_restart=False,
            enable_deduplication=enable_dedup
        )
        print(f"âœ… å¤„ç†å®Œæˆï¼Œç»“æœä¿å­˜åœ¨: {result_file}")

    else:  # analyze
        # æ‰§è¡Œåˆ†æ
        enable_dedup = not args.no_deduplication
        if enable_dedup:
            print("ğŸ“Š å¯ç”¨å»é‡ä¼˜åŒ–ï¼Œç›¸åŒä¸šåŠ¡æ–‡æœ¬åªåˆ†æä¸€æ¬¡")
        else:
            print("âš ï¸  å·²ç¦ç”¨å»é‡ä¼˜åŒ–ï¼Œå°†åˆ†ææ‰€æœ‰é‡å¤æ–‡æœ¬")

        if args.no_incremental:
            print("ğŸš€ å¼€å§‹å®Œæ•´åˆ†æ...")
            result_file = analyzer.analyze_csv_file(
                args.csv_file,
                args.sample_size,
                incremental=False,
                enable_deduplication=enable_dedup
            )
        else:
            print("ğŸš€ å¼€å§‹å¢é‡åˆ†æ...")
            result_file = analyzer.analyze_csv_file_incremental(
                args.csv_file,
                args.sample_size,
                force_restart=args.force_restart,
                enable_deduplication=enable_dedup
            )

        print(f"âœ… APIåˆ†æå®Œæˆï¼Œç»“æœä¿å­˜åœ¨: {result_file}")

        # æ˜¾ç¤ºæœ€ç»ˆçŠ¶æ€
        print("\nğŸ“Š æœ€ç»ˆçŠ¶æ€:")
        analyzer.print_processing_status(args.csv_file, args.sample_size)

if __name__ == "__main__":
    main()
